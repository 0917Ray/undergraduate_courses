{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c6173fd-5be5-467d-b4cb-bc3654260621",
   "metadata": {},
   "source": [
    "Demo code for PINNs \\\n",
    "Author: Changrui Fang from Wuhan University\\\n",
    "2021300002027@whu.edu.cn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7a3458-b162-43de-a2c8-a6451ea37768",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# 1. Base definition -- a DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e36ba336-677b-4a42-afc9-d09347a2320b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import the necessary package\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib as plt\n",
    "import numpy as np\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4e80ffe4-97c7-4b34-ae38-352865015b34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define a DNN \n",
    "\n",
    "class DNN(nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super().__init__()  # initialize the DNN\n",
    "        \n",
    "        # next define the construction of DNN\n",
    "        \n",
    "        self.DEP = len(layers) - 1  # the depth of the network is defined as number of all layers - 1\n",
    "        self.ACT = torch.nn.Tanh  # the activate function of the network we use is Tanh\n",
    "        self.IN = torch.nn.Linear(num_IN, num_Hidden)  # input layer\n",
    "        \n",
    "        layer_list = list()  # difine a empty list firstly\n",
    "        \n",
    "        # next define DNN except last layer\n",
    "        for i in range(self.DEP - 1):\n",
    "            layer_list.append(\n",
    "                ('layer_%d' % i, torch.nn.Linear(layer[i], layer[i+1]))\n",
    "            )\n",
    "            layer_list.append(\n",
    "            ('activation_%d' % i, self.ACT())\n",
    "            )\n",
    "            \n",
    "        # define the last layer\n",
    "        layer_list.append(\n",
    "            ('layer_%d' % (self.DEP -1), torch.nn.Linear(layers[-2], layers[-1]))\n",
    "        )\n",
    "        \n",
    "        layerDict = OrderedDict(layer_list)\n",
    "        \n",
    "        self.layers = torch.nn.Sequential(layerDict)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        outputs = self.layers(inputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d03c55-c80c-4b2f-9114-bf770656872f",
   "metadata": {},
   "source": [
    "# 2. Specific examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0157455c-a436-4e48-8543-91136042dcab",
   "metadata": {},
   "source": [
    "## 2.1 1D poisson problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1823566a-01aa-4024-a075-2cf9ccbb7425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define physis-informed neural network\n",
    "\n",
    "class PINN():\n",
    "    def __init__(self, X, u, layers, lb, ub):\n",
    "        '''\n",
    "        X: Input data\n",
    "        layers: Structure of a neural network\n",
    "        lb: low boundary condition\n",
    "        ub: up boundary condition\n",
    "        '''\n",
    "\n",
    "        # define boundary conditions\n",
    "        self.lb = torch.tensor(lb).float().to(device)\n",
    "        self.ub = torch.tensor(ub).float().to(device)\n",
    "        \n",
    "        \n",
    "        # data\n",
    "        self.x = torch.tensor(X, requires_grad=True).float().to(device)\n",
    "        \n",
    "        # deep neural networks\n",
    "        self.dnn = DNN(layers).to(device)\n",
    "        \n",
    "        # optimizer: Adam\n",
    "        self.Adam = torch.optim.Adam(self.dnn.parameters())\n",
    "        \n",
    "        self.LBFGS = torch.optim.LBFGS(\n",
    "            self.dnn.parameters(),  # 优化的模型参数为神经网络中的权重和偏置\n",
    "            lr=1,  # 学习率\n",
    "            max_iter=50000,  # L-BFGS算法的最大迭代次数\n",
    "            max_eval=50000,  # 允许评估目标函数的最大次数\n",
    "            history_size=50,  # L-BFGS算法历史缓存的大小\n",
    "\n",
    "            # 停机准则\n",
    "            tolerance_grad=1e-8,  # 梯度收敛容差，当梯度的L2范数小于此值时，算法停止\n",
    "            tolerance_change=10 * np.finfo(float).eps,  # 参数变化的容差，当参数的变化小于此值时，算法停止\n",
    "            \n",
    "            line_search_fn=\"strong_wolfe\"       # 线搜索准则： 强 wolfe 准则\n",
    "        )\n",
    "        \n",
    "        self.iter = 0  # itration\n",
    "        \n",
    "    def net_u(self, x):\n",
    "        u = self.dnn(x)\n",
    "        return u\n",
    "    \n",
    "    def net_f(self, x):\n",
    "        u = self.net_u(x)\n",
    "        \n",
    "        u_x = torch.autograd.grad(\n",
    "            u, x, \n",
    "            grad_outputs=torch.ones_like(u),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "\n",
    "        u_xx = torch.autograd.grad(\n",
    "            u_x, x, \n",
    "            grad_outputs=torch.ones_like(u_x),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "        \n",
    "        f = np.pi**2*torch.sin(np.pi*x) - u_xx  # poisson equation: u'' = pi^2 * sin(pi*x)\n",
    "        \n",
    "        return f\n",
    "    \n",
    "    def DataLoss(self):  # data loss\n",
    "\n",
    "        u_pre = self.net_u(self.x)\n",
    "        loss = torch.mean(torch.square(u_pre - self.u))\n",
    "        return loss\n",
    "    \n",
    "    def EqLoss(self):  # equation loss, s.t. physics information\n",
    "        \n",
    "        f_pre = self.net_f(self.x)\n",
    "        loss = torch.mean(torch.square(f_pre))\n",
    "        return loss\n",
    "        \n",
    "    def Loss(self):\n",
    "        \n",
    "        loss = self.DataLoss() + self.EqLoss()\n",
    "        \n",
    "        self.optimizer.zero_grad()  # clear the gradient\n",
    "        loss.backward()  # backpropagation\n",
    "        \n",
    "        self.iter += 1\n",
    "        if (self.iter + 1) % 100 == 0:\n",
    "            print(\n",
    "                'iteration：%.1f, Loss: %e, a: %.5f' %\n",
    "                (\n",
    "                    self.iter+1,\n",
    "                    loss.item()\n",
    "                )\n",
    "            )\n",
    "        return loss\n",
    "    \n",
    "    # next define the train part\n",
    "    def train(self, num_epoch):\n",
    "        self.dnn.train()\n",
    "        \n",
    "        for epoch in range(num_epoch):\n",
    "            u_pred = self.net_u(self.x, self.t)\n",
    "            f_pred = self.net_f(self.x, self.t)\n",
    "            \n",
    "            loss = self.Loss()\n",
    "            \n",
    "            self.optimizer_Adam.zero_grad()\n",
    "            loss.backward\n",
    "            self.optimizer_Adam.step()\n",
    "            \n",
    "            if (epoch+1) % 100 == 0:\n",
    "                print(\n",
    "                    'epoch: %d, Loss: %.3e' % \n",
    "                    (\n",
    "                        epoch+1, \n",
    "                        loss.item()\n",
    "                    )\n",
    "                )\n",
    "                \n",
    "            \n",
    "        # ues L-BFGS to optimize in the last\n",
    "        # self.optimizer.step(self.loss_func)\n",
    "        \n",
    "        def predict(self, X):\n",
    "            x = torch.tensor(X, requires_grad=True).float().to(device)\n",
    "\n",
    "            self.dnn.eval()\n",
    "            u = self.net_u(x, t) # calculate us\n",
    "            f = self.net_f(x, t) # calculate f\n",
    "            u = u.detach().cpu().numpy()  # turn u to NumPy\n",
    "            f = f.detach().cpu().numpy()  # turn f to NumPy\n",
    "            return u, f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b8d9a28-b043-47ec-96f9-4b956ae53179",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 13/12/2023\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# generate training points\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m100\u001b[39m)\n\u001b[0;32m      4\u001b[0m X \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(X,stop_gradient\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      5\u001b[0m X \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mreshape([\u001b[38;5;241m100\u001b[39m,\u001b[38;5;241m1\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# 13/12/2023\n",
    "# generate training points\n",
    "X = np.linspace(-1,1,100)\n",
    "X = torch.tensor(X,stop_gradient=False,dtype='float32').to(device)\n",
    "X = X.reshape([100,1])\n",
    "X_bc = np.linspace(-1,1,2)\n",
    "X_bc = torch.tensor(X_bc,stop_gradient=True,dtype='float32')\n",
    "X_bc = X_bc.reshape([2,1])\n",
    "Y_bc = torch.zeros([2,1],dtype='float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5a24f1-b2af-4f62-a844-8c35a5008024",
   "metadata": {},
   "source": [
    "# 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8ea638-8ff8-47fe-95f8-575cf041fab4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
